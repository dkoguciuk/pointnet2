{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import socket\n",
    "import random\n",
    "import argparse\n",
    "import importlib\n",
    "import statistics\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "%matplotlib inline\n",
    "\n",
    "# Append python path\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "\n",
    "# Import pointnet stuff\n",
    "import provider\n",
    "import modelnet_dataset\n",
    "import modelnet_h5_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_POINT = 1024\n",
    "MODEL_DIR = 'logs_shapenet/log_'\n",
    "GPU_INDEX = 0\n",
    "MODEL = importlib.import_module('pointnet2_cls_ssg') # import network module\n",
    "DUMP_DIR = 'dump'\n",
    "VISUALIZATION = False\n",
    "if not os.path.exists(DUMP_DIR):\n",
    "    os.mkdir(DUMP_DIR)\n",
    "    \n",
    "HOSTNAME = socket.gethostname()\n",
    "NUM_CLASSES = 55\n",
    "SHAPE_NAMES = [line.rstrip() for line in \\\n",
    "    open(os.path.join(BASE_DIR, 'data/shapenet_core55_1024/shape_names.txt'))] \n",
    "\n",
    "# ShapenetCore55 train/test split\n",
    "TRAIN_DATASET = modelnet_h5_dataset.ModelNetH5Dataset(os.path.join(BASE_DIR, 'data/shapenet_core55_1024/train_files.txt'), batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=True)\n",
    "TEST_DATASET = modelnet_h5_dataset.ModelNetH5Dataset(os.path.join(BASE_DIR, 'data/shapenet_core55_1024/test_files.txt'), batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE ONE EPOCH METHOD\n",
    "\n",
    "Args:\n",
    "\n",
    "  **sess** - tf session\n",
    "  \n",
    "  **ops** - parameters\n",
    "  \n",
    "  \n",
    "\n",
    "Returns:\n",
    "\n",
    "  **loss** - mean classification loss\n",
    "  \n",
    "  **accuracy** - instance classification accuracy\n",
    "  \n",
    "  **class_accuracy** - class classification accuracy\n",
    "  \n",
    "  **predictions** - array of the output ofthe classification module with shape: (N, 40), where N is the test clouds len\n",
    "  \n",
    "  **true_labels** - true labels of test clouds with the lenght of N\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(sess, ops, num_votes=1, topk=1, verbose=False):\n",
    "    is_training = False\n",
    "\n",
    "    # Make sure batch data is of same size\n",
    "    cur_batch_data = np.zeros((BATCH_SIZE,NUM_POINT,TEST_DATASET.num_channel()))\n",
    "    cur_batch_label = np.zeros((BATCH_SIZE), dtype=np.int32)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    batch_idx = 0\n",
    "    shape_ious = []\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    fout = open(os.path.join(DUMP_DIR, 'pred_label.txt'), 'w')\n",
    "    pred_vals = []\n",
    "    true_vals = []\n",
    "    TEST_DATASET.reset()\n",
    "    while TEST_DATASET.has_next_batch():\n",
    "        batch_data, batch_label = TEST_DATASET.next_batch(augment=False)\n",
    "        bsize = batch_data.shape[0]\n",
    "        if verbose:\n",
    "            print('Batch: %03d, batch size: %d'%(batch_idx, bsize))\n",
    "        # for the last batch in the epoch, the bsize:end are from last batch\n",
    "        cur_batch_data[0:bsize,...] = batch_data\n",
    "        cur_batch_label[0:bsize] = batch_label\n",
    "\n",
    "        batch_pred_sum = np.zeros((BATCH_SIZE, NUM_CLASSES)) # score for classes\n",
    "        for vote_idx in range(num_votes):\n",
    "            # Shuffle point order to achieve different farthest samplings\n",
    "            shuffled_indices = np.arange(NUM_POINT)\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            rotated_data = provider.rotate_point_cloud_by_angle(cur_batch_data[:, shuffled_indices, :],\n",
    "                    vote_idx/float(num_votes) * np.pi * 2)\n",
    "            feed_dict = {ops['pointclouds_pl']: rotated_data,\n",
    "                         ops['labels_pl']: cur_batch_label,\n",
    "                         ops['is_training_pl']: is_training}\n",
    "            loss_val, pred_val = sess.run([ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "            batch_pred_sum += pred_val\n",
    "       \n",
    "        # DANIEL\n",
    "        pred_vals.append(batch_pred_sum[0:bsize].copy())\n",
    "        true_vals.append(cur_batch_label[0:bsize].copy())\n",
    "        \n",
    "        # argmax\n",
    "        pred_val = np.argmax(batch_pred_sum, 1)   \n",
    "        correct = np.sum(pred_val[0:bsize] == batch_label[0:bsize])\n",
    "\n",
    "        # total values\n",
    "        total_correct += correct\n",
    "        total_seen += bsize\n",
    "        loss_sum += loss_val\n",
    "        for i in range(bsize):\n",
    "            l = batch_label[i]\n",
    "            total_seen_class[l] += 1\n",
    "            total_correct_class[l] += (pred_val[i] == l)\n",
    "    \n",
    "    eval_mean_loss = (loss_sum / float(total_seen))\n",
    "    eval_instance_acc = (total_correct / float(total_seen))\n",
    "    eval_class_acc = (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float)))\n",
    "    \n",
    "    pred_vals = np.concatenate(pred_vals)\n",
    "    true_vals = np.concatenate(true_vals)\n",
    "    \n",
    "    if verbose:\n",
    "        print('eval mean loss: %f' % eval_mean_loss)\n",
    "        print('eval accuracy: %f' % eval_instance_acc)\n",
    "        print('eval avg class acc: %f' % eval_class_acc)\n",
    "    \n",
    "    class_accuracies = np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float)\n",
    "    if verbose:\n",
    "        for i, name in enumerate(SHAPE_NAMES):\n",
    "            print('%10s:\\t%0.3f' % (name, class_accuracies[i]))\n",
    "            \n",
    "    return eval_mean_loss, eval_instance_acc, eval_class_acc, pred_vals, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE METHOD\n",
    "\n",
    "Args:\n",
    "\n",
    "  **model_num** - number of the pointnet model (i.e.: 3 to evaluate model_3.ckpt model)\n",
    "  \n",
    "  **num_votes** - how many votes (one vote is one pc rotation & permutation) should be used to eval the model\n",
    "\n",
    "Returns:\n",
    "\n",
    "  ** same as *eval_one_epoch* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_num, num_votes, verbose=False):\n",
    "    is_training = False\n",
    "    \n",
    "    # Reset\n",
    "    tf.reset_default_graph()\n",
    "     \n",
    "    with tf.device('/GPU:'+str(GPU_INDEX)):\n",
    "        pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "        is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "        # simple model\n",
    "        pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, num_classes=NUM_CLASSES)\n",
    "        MODEL.get_loss(pred, labels_pl, end_points)\n",
    "        losses = tf.get_collection('losses')\n",
    "        total_loss = tf.add_n(losses, name='total_loss')\n",
    "\n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Create a session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    config.log_device_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    # Get model path\n",
    "    model_path = os.path.join(MODEL_DIR + str(model_num), 'model.ckpt')\n",
    "    \n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model_path)\n",
    "    if verbose:\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "    ops = {'pointclouds_pl': pointclouds_pl,\n",
    "           'labels_pl': labels_pl,\n",
    "           'is_training_pl': is_training_pl,\n",
    "           'pred': pred,\n",
    "           'loss': total_loss}\n",
    "\n",
    "    return eval_one_epoch(sess, ops, num_votes, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_0/model.ckpt\n",
      "('acc =', 0.8954273219887822, 'calc time [min] =', 2.6002861499786376)\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "loss, instance_acc, class_acc, pred_vals, true_vals = evaluate(model_num=0, num_votes=1, verbose=False)\n",
    "end = timer()\n",
    "print ('acc =', instance_acc, 'calc time [min] =', (end-start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# NUM_VOTES TEST\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOTES = 15\n",
    "RANGE_MODELS = range(1, 11)\n",
    "num_votes_accs = {i: [] for i in RANGE_MODELS}\n",
    "for i in num_votes_accs:\n",
    "    for x in range(1, NUM_VOTES+1):\n",
    "        _, acc, _, _, _ = evaluate(model_num=i, num_votes=x, verbose=False)\n",
    "        num_votes_accs[i].append(acc)\n",
    "        print ('i=', i, 'x=', x, 'acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in num_votes_accs:\n",
    "    plt.plot(np.arange(1, 1+len(num_votes_accs[i])), num_votes_accs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_votes_accs_np = np.zeros((max(RANGE_MODELS), NUM_VOTES), dtype=np.float)\n",
    "for i in num_votes_accs:\n",
    "    num_votes_accs_np[i-1] = num_votes_accs[i]\n",
    "np.save('log/num_votes_accs.npy', num_votes_accs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# MODEL ENSEMBLING \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "## Calculate probabilities for each test cloud and each model. The output probability array will be the shape of (N, 40, X), where N is the test cloud len and X is the models count to be ensembled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_0/model.ckpt\n",
      "('Model =', 0, 'acc =', 0.8964730487688943, 'time [min] =', 16.003492951393127)\n",
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_1/model.ckpt\n",
      "('Model =', 1, 'acc =', 0.8959026523433786, 'time [min] =', 15.597500018278758)\n",
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_2/model.ckpt\n",
      "('Model =', 2, 'acc =', 0.8956174541306208, 'time [min] =', 15.889894898732503)\n",
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_3/model.ckpt\n",
      "('Model =', 3, 'acc =', 0.8972335773362486, 'time [min] =', 16.044625119368234)\n",
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_4/model.ckpt\n",
      "('Model =', 4, 'acc =', 0.8960927844852172, 'time [min] =', 16.10136084953944)\n",
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_5/model.ckpt\n",
      "('Model =', 5, 'acc =', 0.8968533130525715, 'time [min] =', 16.076002816359203)\n",
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_6/model.ckpt\n",
      "('Model =', 6, 'acc =', 0.8978039737617645, 'time [min] =', 16.117261282602946)\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "logs_shapenet/log_7; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a33b54239e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_MODELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_votes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_VOTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprobabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f557f9d5dc36>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model_num, num_votes, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Restore variables from disk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1713\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[1;32m   1717\u001b[0m                        + compat.as_text(save_path))\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m   2054\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m   2055\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m-> 2056\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    342\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m           for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[0;32m--> 344\u001b[0;31m               compat.as_bytes(single_filename), status)\n\u001b[0m\u001b[1;32m    345\u001b[0m       ]\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: logs_shapenet/log_7; No such file or directory"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 10\n",
    "NUM_VOTES = 12\n",
    "probabilities = []\n",
    "true_labels = []\n",
    "accuracies = []\n",
    "for x in range(NUM_MODELS):\n",
    "    start = timer()\n",
    "    loss, instance_acc, class_acc, pred_vals, true_vals = evaluate(model_num=x, num_votes=NUM_VOTES, verbose=False)\n",
    "    end = timer()\n",
    "    probabilities.append(pred_vals)\n",
    "    accuracies.append(instance_acc)\n",
    "    true_labels = np.array(true_vals)\n",
    "    print ('Model =', x, 'acc =', instance_acc, 'time [min] =', (end-start)/60)\n",
    "\n",
    "probabilities = np.stack(probabilities).transpose(1, 2, 0)\n",
    "accuracies = np.array(accuracies)\n",
    "\n",
    "np.save('logs_shapenet/probabilities.npy', probabilities)\n",
    "np.save('logs_shapenet/true_labels.npy', true_labels)\n",
    "np.save('logs_shapenet/accuracies.npy', accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_7/model.ckpt\n",
      "('Model =', 7, 'acc =', 0.8987546344709573, 'time [min] =', 15.883458085854848)\n",
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_8/model.ckpt\n",
      "('Model =', 8, 'acc =', 0.8974237094780873, 'time [min] =', 15.990689218044281)\n",
      "INFO:tensorflow:Restoring parameters from logs_shapenet/log_9/model.ckpt\n",
      "('Model =', 9, 'acc =', 0.8958075862724594, 'time [min] =', 16.11862374941508)\n"
     ]
    }
   ],
   "source": [
    "for x in [7, 8, 9]:\n",
    "    start = timer()\n",
    "    loss, instance_acc, class_acc, pred_vals, true_vals = evaluate(model_num=x, num_votes=NUM_VOTES, verbose=False)\n",
    "    end = timer()\n",
    "    probabilities.append(pred_vals)\n",
    "    accuracies.append(instance_acc)\n",
    "    true_labels = np.array(true_vals)\n",
    "    print ('Model =', x, 'acc =', instance_acc, 'time [min] =', (end-start)/60)\n",
    "\n",
    "probabilities = np.stack(probabilities).transpose(1, 2, 0)\n",
    "accuracies = np.array(accuracies)\n",
    "\n",
    "np.save('logs_shapenet/probabilities.npy', probabilities)\n",
    "np.save('logs_shapenet/true_labels.npy', true_labels)\n",
    "np.save('logs_shapenet/accuracies.npy', accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean accuracy =', 0.8967962734100199)\n",
      "('Mean of validation max results =', 0.8833276018099546)\n",
      "('Mean of test result for validation max =', 0.8799586578789694)\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy =', statistics.mean(accuracies))\n",
    "indices = {}\n",
    "for k in range(40):\n",
    "    indices[k] = [i for i, x in enumerate(true_labels) if x == k]\n",
    "    \n",
    "validation_max_res = []\n",
    "test_res_at_validation_max = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    validation_indices = []\n",
    "    test_indices = []\n",
    "    for k in indices:\n",
    "        random.shuffle(indices[k])\n",
    "        split_idx = int(len(indices[k])/2)\n",
    "        validation_indices += indices[k][:split_idx]\n",
    "        test_indices += indices[k][split_idx:]\n",
    "    validation_indices = sorted(validation_indices)\n",
    "    test_indices = sorted(test_indices)\n",
    "\n",
    "    validation_true_labels = true_labels[validation_indices]\n",
    "    validation_probabilities = probabilities[validation_indices]\n",
    "    test_true_labels = true_labels[test_indices]\n",
    "    test_probabilities = probabilities[test_indices]\n",
    "\n",
    "    validation_predictions = np.argmax(validation_probabilities, axis=1)\n",
    "    validation_compare = np.equal(validation_predictions, np.expand_dims(validation_true_labels, -1))\n",
    "    validation_accuracies = np.mean(validation_compare, axis=0)\n",
    "\n",
    "    test_predictions = np.argmax(test_probabilities, axis=1)\n",
    "    test_compare = np.equal(test_predictions, np.expand_dims(test_true_labels, -1))\n",
    "    test_accuracies = np.mean(test_compare, axis=0)\n",
    "\n",
    "    validation_max_res.append(np.max(validation_accuracies))\n",
    "    test_res_at_validation_max.append(test_accuracies[np.argmax(validation_accuracies)])\n",
    "    \n",
    "mean_valid_max = statistics.mean(validation_max_res)\n",
    "mean_test_at_valid_max = statistics.mean(test_res_at_validation_max)\n",
    "\n",
    "print('Mean of validation max results =', mean_valid_max)\n",
    "print('Mean of test result for validation max =', mean_test_at_valid_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with sum operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9004658237475045"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_probability = np.sum(probabilities, axis=-1)\n",
    "aggregated_predictions = np.argmax(aggregated_probability, axis=-1)\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with mean operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9004658237475045"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_probability = np.mean(probabilities, axis=-1)\n",
    "aggregated_predictions = np.argmax(aggregated_probability, axis=-1)\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with mode operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899990493392908"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_predictions = np.argmax(probabilities, axis=1)\n",
    "aggregated_predictions =  np.squeeze(stats.mode(aggregated_predictions, axis=1)[0])\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
