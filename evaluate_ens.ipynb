{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import socket\n",
    "import random\n",
    "import argparse\n",
    "import importlib\n",
    "import statistics\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Append python path\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "\n",
    "# Import pointnet stuff\n",
    "import provider\n",
    "import modelnet_dataset\n",
    "import modelnet_h5_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_POINT = 1024\n",
    "MODEL_DIR = 'log'\n",
    "GPU_INDEX = 0\n",
    "MODEL = importlib.import_module('pointnet2_cls_ssg') # import network module\n",
    "DUMP_DIR = 'dump'\n",
    "VISUALIZATION = False\n",
    "if not os.path.exists(DUMP_DIR):\n",
    "    os.mkdir(DUMP_DIR)\n",
    "    \n",
    "HOSTNAME = socket.gethostname()\n",
    "NUM_CLASSES = 40\n",
    "SHAPE_NAMES = [line.rstrip() for line in \\\n",
    "    open(os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/shape_names.txt'))] \n",
    "\n",
    "# ModelNet40 official train/test split\n",
    "TRAIN_DATASET = modelnet_h5_dataset.ModelNetH5Dataset(os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/train_files.txt'), batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=True)\n",
    "TEST_DATASET = modelnet_h5_dataset.ModelNetH5Dataset(os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/test_files.txt'), batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE ONE EPOCH METHOD\n",
    "\n",
    "Args:\n",
    "\n",
    "  **sess** - tf session\n",
    "  \n",
    "  **ops** - parameters\n",
    "  \n",
    "  \n",
    "\n",
    "Returns:\n",
    "\n",
    "  **loss** - mean classification loss\n",
    "  \n",
    "  **accuracy** - instance classification accuracy\n",
    "  \n",
    "  **class_accuracy** - class classification accuracy\n",
    "  \n",
    "  **predictions** - array of the output ofthe classification module with shape: (N, 40), where N is the test clouds len\n",
    "  \n",
    "  **true_labels** - true labels of test clouds with the lenght of N\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(sess, ops, num_votes=1, topk=1, verbose=False):\n",
    "    is_training = False\n",
    "\n",
    "    # Make sure batch data is of same size\n",
    "    cur_batch_data = np.zeros((BATCH_SIZE,NUM_POINT,TEST_DATASET.num_channel()))\n",
    "    cur_batch_label = np.zeros((BATCH_SIZE), dtype=np.int32)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    batch_idx = 0\n",
    "    shape_ious = []\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    fout = open(os.path.join(DUMP_DIR, 'pred_label.txt'), 'w')\n",
    "    pred_vals = []\n",
    "    true_vals = []\n",
    "    TEST_DATASET.reset()\n",
    "    while TEST_DATASET.has_next_batch():\n",
    "        batch_data, batch_label = TEST_DATASET.next_batch(augment=False)\n",
    "        bsize = batch_data.shape[0]\n",
    "        if verbose:\n",
    "            print('Batch: %03d, batch size: %d'%(batch_idx, bsize))\n",
    "        # for the last batch in the epoch, the bsize:end are from last batch\n",
    "        cur_batch_data[0:bsize,...] = batch_data\n",
    "        cur_batch_label[0:bsize] = batch_label\n",
    "\n",
    "        batch_pred_sum = np.zeros((BATCH_SIZE, NUM_CLASSES)) # score for classes\n",
    "        for vote_idx in range(num_votes):\n",
    "            # Shuffle point order to achieve different farthest samplings\n",
    "            shuffled_indices = np.arange(NUM_POINT)\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            rotated_data = provider.rotate_point_cloud_by_angle(cur_batch_data[:, shuffled_indices, :],\n",
    "                    vote_idx/float(num_votes) * np.pi * 2)\n",
    "            feed_dict = {ops['pointclouds_pl']: rotated_data,\n",
    "                         ops['labels_pl']: cur_batch_label,\n",
    "                         ops['is_training_pl']: is_training}\n",
    "            loss_val, pred_val = sess.run([ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "            batch_pred_sum += pred_val\n",
    "       \n",
    "        # DANIEL\n",
    "        pred_vals.append(batch_pred_sum[0:bsize].copy())\n",
    "        true_vals.append(cur_batch_label[0:bsize].copy())\n",
    "        \n",
    "        # argmax\n",
    "        pred_val = np.argmax(batch_pred_sum, 1)   \n",
    "        correct = np.sum(pred_val[0:bsize] == batch_label[0:bsize])\n",
    "\n",
    "        # total values\n",
    "        total_correct += correct\n",
    "        total_seen += bsize\n",
    "        loss_sum += loss_val\n",
    "        for i in range(bsize):\n",
    "            l = batch_label[i]\n",
    "            total_seen_class[l] += 1\n",
    "            total_correct_class[l] += (pred_val[i] == l)\n",
    "    \n",
    "    eval_mean_loss = (loss_sum / float(total_seen))\n",
    "    eval_instance_acc = (total_correct / float(total_seen))\n",
    "    eval_class_acc = (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float)))\n",
    "    \n",
    "    pred_vals = np.concatenate(pred_vals)\n",
    "    true_vals = np.concatenate(true_vals)\n",
    "    \n",
    "    if verbose:\n",
    "        print('eval mean loss: %f' % eval_mean_loss)\n",
    "        print('eval accuracy: %f' % eval_instance_acc)\n",
    "        print('eval avg class acc: %f' % eval_class_acc)\n",
    "    \n",
    "    class_accuracies = np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float)\n",
    "    if verbose:\n",
    "        for i, name in enumerate(SHAPE_NAMES):\n",
    "            print('%10s:\\t%0.3f' % (name, class_accuracies[i]))\n",
    "            \n",
    "    return eval_mean_loss, eval_instance_acc, eval_class_acc, pred_vals, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE METHOD\n",
    "\n",
    "Args:\n",
    "\n",
    "  **model_num** - number of the pointnet model (i.e.: 3 to evaluate model_3.ckpt model)\n",
    "  \n",
    "  **num_votes** - how many votes (one vote is one pc rotation & permutation) should be used to eval the model\n",
    "\n",
    "Returns:\n",
    "\n",
    "  ** same as *eval_one_epoch* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model_num, num_votes, verbose=False):\n",
    "    is_training = False\n",
    "    \n",
    "    # Reset\n",
    "    tf.reset_default_graph()\n",
    "     \n",
    "    with tf.device('/GPU:'+str(GPU_INDEX)):\n",
    "        pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "        is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "        # simple model\n",
    "        pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl)\n",
    "        MODEL.get_loss(pred, labels_pl, end_points)\n",
    "        losses = tf.get_collection('losses')\n",
    "        total_loss = tf.add_n(losses, name='total_loss')\n",
    "\n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Create a session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    config.log_device_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    # Get model path\n",
    "    model_path = os.path.join(MODEL_DIR, 'model_' + str(model_num) + '.ckpt')\n",
    "    \n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model_path)\n",
    "    if verbose:\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "    ops = {'pointclouds_pl': pointclouds_pl,\n",
    "           'labels_pl': labels_pl,\n",
    "           'is_training_pl': is_training_pl,\n",
    "           'pred': pred,\n",
    "           'loss': total_loss}\n",
    "\n",
    "    return eval_one_epoch(sess, ops, num_votes, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# NUM_VOTES TEST\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOTES = 15\n",
    "RANGE_MODELS = range(1, 11)\n",
    "num_votes_accs = {i: [] for i in RANGE_MODELS}\n",
    "for i in num_votes_accs:\n",
    "    for x in range(1, NUM_VOTES+1):\n",
    "        _, acc, _, _, _ = evaluate(model_num=i, num_votes=x, verbose=False)\n",
    "        num_votes_accs[i].append(acc)\n",
    "        print ('i=', i, 'x=', x, 'acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in num_votes_accs:\n",
    "    plt.plot(np.arange(1, 1+len(num_votes_accs[i])), num_votes_accs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_votes_accs_np = np.zeros((max(RANGE_MODELS), NUM_VOTES), dtype=np.float)\n",
    "for i in num_votes_accs:\n",
    "    num_votes_accs_np[i-1] = num_votes_accs[i]\n",
    "np.save('log/num_votes_accs.npy', num_votes_accs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# MODEL ENSEMBLING \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "## Calculate probabilities for each test cloud and each model. The output probability array will be the shape of (N, 40, X), where N is the test cloud len and X is the models count to be ensembled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log/model_1.ckpt\n",
      "('Model =', 1, 'acc = ', 0.9007293354943274)\n",
      "INFO:tensorflow:Restoring parameters from log/model_2.ckpt\n",
      "('Model =', 2, 'acc = ', 0.8970826580226904)\n",
      "INFO:tensorflow:Restoring parameters from log/model_3.ckpt\n",
      "('Model =', 3, 'acc = ', 0.9011345218800648)\n",
      "INFO:tensorflow:Restoring parameters from log/model_4.ckpt\n",
      "('Model =', 4, 'acc = ', 0.9015397082658023)\n",
      "INFO:tensorflow:Restoring parameters from log/model_5.ckpt\n",
      "('Model =', 5, 'acc = ', 0.8958670988654781)\n",
      "INFO:tensorflow:Restoring parameters from log/model_6.ckpt\n",
      "('Model =', 6, 'acc = ', 0.9064019448946515)\n",
      "INFO:tensorflow:Restoring parameters from log/model_7.ckpt\n",
      "('Model =', 7, 'acc = ', 0.9059967585089141)\n",
      "INFO:tensorflow:Restoring parameters from log/model_8.ckpt\n",
      "('Model =', 8, 'acc = ', 0.9015397082658023)\n",
      "INFO:tensorflow:Restoring parameters from log/model_9.ckpt\n",
      "('Model =', 9, 'acc = ', 0.9035656401944895)\n",
      "INFO:tensorflow:Restoring parameters from log/model_10.ckpt\n",
      "('Model =', 10, 'acc = ', 0.8999189627228525)\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 10\n",
    "NUM_VOTES = 12\n",
    "probabilities = []\n",
    "true_labels = []\n",
    "accuracies = []\n",
    "for x in range(1, NUM_MODELS+1):\n",
    "    _, acc, _, pred_vals, true_vals = evaluate(model_num=x, num_votes=NUM_VOTES, verbose=False)\n",
    "    probabilities.append(pred_vals)\n",
    "    accuracies.append(acc)\n",
    "    true_labels = np.array(true_vals)\n",
    "    print ('Model =', x, 'acc = ', acc)\n",
    "    \n",
    "probabilities = np.stack(probabilities).transpose(1, 2, 0)\n",
    "accuracies = np.array(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('log/probabilities.npy', probabilities)\n",
    "np.save('log/true_labels.npy', true_labels)\n",
    "np.save('log/accuracies.npy', accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean accuracy =', 0.9013776337115074)\n",
      "('Mean of validation max results =', 0.9076742301458671)\n",
      "('Mean of test result for validation max =', 0.9036750405186386)\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy =', statistics.mean(accuracies))\n",
    "indices = {}\n",
    "for k in range(40):\n",
    "    indices[k] = [i for i, x in enumerate(true_labels) if x == k]\n",
    "    \n",
    "validation_max_res = []\n",
    "test_res_at_validation_max = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    validation_indices = []\n",
    "    test_indices = []\n",
    "    for k in indices:\n",
    "        random.shuffle(indices[k])\n",
    "        split_idx = int(len(indices[k])/2)\n",
    "        validation_indices += indices[k][:split_idx]\n",
    "        test_indices += indices[k][split_idx:]\n",
    "    validation_indices = sorted(validation_indices)\n",
    "    test_indices = sorted(test_indices)\n",
    "\n",
    "    validation_true_labels = true_labels[validation_indices]\n",
    "    validation_probabilities = probabilities[validation_indices]\n",
    "    test_true_labels = true_labels[test_indices]\n",
    "    test_probabilities = probabilities[test_indices]\n",
    "\n",
    "    validation_predictions = np.argmax(validation_probabilities, axis=1)\n",
    "    validation_compare = np.equal(validation_predictions, np.expand_dims(validation_true_labels, -1))\n",
    "    validation_accuracies = np.mean(validation_compare, axis=0)\n",
    "\n",
    "    test_predictions = np.argmax(test_probabilities, axis=1)\n",
    "    test_compare = np.equal(test_predictions, np.expand_dims(test_true_labels, -1))\n",
    "    test_accuracies = np.mean(test_compare, axis=0)\n",
    "\n",
    "    validation_max_res.append(np.max(validation_accuracies))\n",
    "    test_res_at_validation_max.append(test_accuracies[np.argmax(validation_accuracies)])\n",
    "    \n",
    "mean_valid_max = statistics.mean(validation_max_res)\n",
    "mean_test_at_valid_max = statistics.mean(test_res_at_validation_max)\n",
    "\n",
    "print('Mean of validation max results =', mean_valid_max)\n",
    "print('Mean of test result for validation max =', mean_test_at_valid_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with sum operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9047811993517018"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_probability = np.sum(probabilities, axis=-1)\n",
    "aggregated_predictions = np.argmax(aggregated_probability, axis=-1)\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with mean operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9047811993517018"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_probability = np.mean(probabilities, axis=-1)\n",
    "aggregated_predictions = np.argmax(aggregated_probability, axis=-1)\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with mode operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9072123176661264"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_predictions = np.argmax(probabilities, axis=1)\n",
    "aggregated_predictions =  np.squeeze(stats.mode(aggregated_predictions, axis=1)[0])\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
